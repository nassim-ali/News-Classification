{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n",
    "from pyspark.sql import SparkSession # to initiate spark\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "from pyspark.ml.feature import HashingTF, IDF # vectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover # to remove stop words\n",
    "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
    "from pyspark.ml.classification import LogisticRegression # ml model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # to evaluate the model\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Younoussa:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>news-classification</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23b52b5b9e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"news-classification\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|Class Index|               Title|         Description|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          3|Wall St. Bears Cl...|Reuters - Short-s...|\n",
      "|          3|Carlyle Looks Tow...|Reuters - Private...|\n",
      "|          3|Oil and Economy C...|Reuters - Soaring...|\n",
      "|          3|Iraq Halts Oil Ex...|Reuters - Authori...|\n",
      "|          3|Oil prices soar t...|AFP - Tearaway wo...|\n",
      "+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/news-dataset.csv\", inferSchema=True, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show 5 first rows with null values\n",
    "df.filter(df['Class Index'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming 'Class Index' col to 'label'\n",
    "df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "# Add a new column 'Text' by concatinating 'Title' and 'Description'\n",
    "df = df.withColumn(\"Text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "# Remove old text columns\n",
    "df = df.select('label', 'Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label='3', Text=\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                Text|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    3|Wall St. Bears Cl...|[wall, st, bears,...|\n",
      "|    3|Carlyle Looks Tow...|[carlyle, looks, ...|\n",
      "|    3|Oil and Economy C...|[oil, and, econom...|\n",
      "|    3|Iraq Halts Oil Ex...|[iraq, halts, oil...|\n",
      "|    3|Oil prices soar t...|[oil, prices, soa...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert sentences to list of words\n",
    "tokenizer = RegexTokenizer(inputCol=\"Text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# adds a column 'words' to df after tokenization\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "df.select(['label','Text', 'words']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label='3', Text=\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", words=['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 's', 'dwindling', 'band', 'of', 'ultra', 'cynics', 'are', 'seeing', 'green', 'again'], filtered=['wall', 'st', 'bears', 'claw', 'back', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 'dwindling', 'band', 'ultra', 'cynics', 'seeing', 'green'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove stop words like is, the, in, etc.\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# adds a column 'filtered' to df without stopwords\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "df.select(['label','Text', 'words', 'filtered']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate term frequency in each article\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\", \n",
    "                       numFeatures=10000)\n",
    "\n",
    "# adds raw tf features to df\n",
    "featurized_data = hashing_tf.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label='3', Text=\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", words=['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 's', 'dwindling', 'band', 'of', 'ultra', 'cynics', 'are', 'seeing', 'green', 'again'], filtered=['wall', 'st', 'bears', 'claw', 'back', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 'dwindling', 'band', 'ultra', 'cynics', 'seeing', 'green'], raw_features=SparseVector(10000, {551: 1.0, 662: 1.0, 1262: 1.0, 1449: 1.0, 1889: 1.0, 1948: 1.0, 2503: 1.0, 2826: 1.0, 3038: 1.0, 3684: 1.0, 4443: 1.0, 6404: 2.0, 8318: 1.0, 8430: 1.0, 8450: 2.0, 9430: 1.0}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label='3', Text=\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", words=['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 's', 'dwindling', 'band', 'of', 'ultra', 'cynics', 'are', 'seeing', 'green', 'again'], filtered=['wall', 'st', 'bears', 'claw', 'back', 'black', 'reuters', 'reuters', 'short', 'sellers', 'wall', 'street', 'dwindling', 'band', 'ultra', 'cynics', 'seeing', 'green'], features=SparseVector(10000, {551: 5.0673, 662: 6.0128, 1262: 4.8019, 1449: 5.9636, 1889: 6.9275, 1948: 4.9649, 2503: 7.6128, 2826: 5.9398, 3038: 4.6635, 3684: 4.3958, 4443: 5.7126, 6404: 9.1681, 8318: 8.034, 8430: 3.394, 8450: 4.6006, 9430: 6.4834}))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse document frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "# converting text to vectors\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "\n",
    "# top 20 rows\n",
    "rescaled_data.select(\"label\",'Text', 'words', 'filtered', \"features\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaled_data.filter(rescaled_data['label'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data = rescaled_data.coalesce(1)\n",
    "(train, test) = rescaled_data.randomSplit([0.80, 0.20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------+------------+--------+\n",
      "|label|Text|words|filtered|raw_features|features|\n",
      "+-----+----+-----+--------+------------+--------+\n",
      "+-----+----+-----+--------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_with_nulls = train.filter(train[\"label\"].isNull())\n",
    "train_with_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 103720\n",
      "Test Dataset Count: 25813\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.withColumn(\"label\", train[\"label\"].cast(DoubleType()))\n",
    "test = test.withColumn(\"label\", test[\"label\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3881"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train['label'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|                Text|               words|            filtered|        raw_features|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1.0| #36;350,000 Siph...|[36, 350, 000, si...|[36, 350, 000, si...|(10000,[157,524,5...|(10000,[157,524,5...|\n",
      "|  1.0| #36;71M Judgment...|[36, 71m, judgmen...|[36, 71m, judgmen...|(10000,[531,633,1...|(10000,[531,633,1...|\n",
      "|  1.0| #39;6-Way Talks ...|[39, 6, way, talk...|[39, 6, way, talk...|(10000,[376,446,5...|(10000,[376,446,5...|\n",
      "|  1.0| #39;70,000 Darfu...|[39, 70, 000, dar...|[39, 70, 000, dar...|(10000,[55,130,49...|(10000,[55,130,49...|\n",
      "|  1.0| #39;9-11 helper ...|[39, 9, 11, helpe...|[39, 9, 11, helpe...|(10000,[132,626,6...|(10000,[132,626,6...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove nulls from train\n",
    "train = train.na.drop()\n",
    "test = test.na.drop()\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.filter(train['label'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=50)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test set\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----+\n",
      "|                Text|         probability|prediction|label|\n",
      "+--------------------+--------------------+----------+-----+\n",
      "| #39;Batman #39; ...|[9.94715584818671...|       1.0|  1.0|\n",
      "| #39;Black boxes ...|[1.00056389661963...|       1.0|  1.0|\n",
      "| #39;Chemical Ali...|[5.78458034240974...|       1.0|  1.0|\n",
      "| #39;Deserter #39...|[6.27428663807804...|       1.0|  1.0|\n",
      "| #39;Ethnic viole...|[6.23124782322524...|       1.0|  1.0|\n",
      "| #39;Hundreds #39...|[3.14086818099292...|       1.0|  1.0|\n",
      "| #39;Hurriyat get...|[1.15737679612712...|       4.0|  1.0|\n",
      "| #39;IAEA may hav...|[6.00777986000207...|       1.0|  1.0|\n",
      "| #39;Militant #39...|[2.30651369034430...|       1.0|  1.0|\n",
      "| #39;Miracle #39;...|[1.23534646179662...|       1.0|  1.0|\n",
      "+--------------------+--------------------+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show top 10 predictions\n",
    "predictions.select(\"Text\", 'probability','prediction', 'label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.28%\n"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "#accuracy in percentage\n",
    "accuracy = evaluator.evaluate(predictions)*100\n",
    "print(\"Accuracy: \" + str(accuracy.__round__(2)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function that process a given news\n",
    "def process_text(news):\n",
    "    df = spark.createDataFrame([(news,)], schema=[\"news\"]) # create df from news (news)\n",
    "    tokenizer = RegexTokenizer(inputCol=\"news\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "    df = tokenizer.transform(df)\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    df = stopwords_remover.transform(df)\n",
    "    hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                           outputCol=\"raw_features\", \n",
    "                           numFeatures=10000)\n",
    "    featurized_data = hashing_tf.transform(df)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idf_vectorizer = idf.fit(featurized_data)\n",
    "    rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "    return rescaled_data\n",
    "\n",
    "def predict(news):\n",
    "    df = process_text(news)\n",
    "    predictions = lrModel.transform(df)\n",
    "    return predictions\n",
    "\n",
    "predictions = predict(\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
    "predictions.select(\"Text\", 'probability','prediction', 'label').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data =[(\"Hello World\",)]\n",
    "columns = [\"news\"]\n",
    "df1 = spark.createDataFrame(string_data, columns)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"World\", \"Sports\", \"Business\",\"Science\", \"Health\",\"Politics\",\"Entertainment\"]\n",
    "\n",
    "# important: need to cast to float type, and order by prediction, else it won't work\n",
    "preds_and_labels = predictions.select(['prediction','label']) \\\n",
    "                              .withColumn('label', col('label') \\\n",
    "                              ) \\\n",
    "                              .orderBy('prediction')\n",
    "\n",
    "preds_and_labels_rdd = preds_and_labels.rdd.map(lambda row: (row['prediction'], row['label']))\n",
    "\n",
    "# generate metrics\n",
    "metrics = MulticlassMetrics(preds_and_labels_rdd)\n",
    "\n",
    "# figure object\n",
    "_ = plt.figure(figsize=(7, 7))\n",
    "\n",
    "# plot confusion matrix\n",
    "sns.heatmap(metrics.confusionMatrix().toArray(),\n",
    "            cmap='viridis',\n",
    "            annot=True,fmt='0',\n",
    "            cbar=False, \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(0.0, 0.0), (1.0, 1.0), (2.0, 2.0), (1.0, 0.0)]\n",
    "predictions = spark.createDataFrame(data, [\"prediction\", \"label\"])\n",
    "\n",
    "# Transforme en RDD\n",
    "preds_and_labels_rdd = predictions.rdd.map(lambda row: (float(row['prediction']), float(row['label'])))\n",
    "\n",
    "# Testez l'action\n",
    "print(preds_and_labels_rdd.take(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value\"])\n",
    "print(df.show())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
